{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# sci-kit learn libraries\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# set seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "\n",
    "# import training data set\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "train_df['_data'] = 'train'\n",
    "dfs['train'] = train_df\n",
    "\n",
    "# import test data set\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "test_df['_data'] = 'test'\n",
    "dfs['test'] = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine train and test data\n",
    "combined_df = dfs['train'].append(dfs['test'])\n",
    "\n",
    "# lowercase column names\n",
    "combined_df.columns = map(str.lower, combined_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parse datetime column & add new time related columns\n",
    "dt = pd.DatetimeIndex(combined_df['datetime'])\n",
    "combined_df.set_index(dt, inplace=True)\n",
    "\n",
    "# create new columns for day, month, year, hour\n",
    "combined_df['date'] = dt.date\n",
    "combined_df['day'] = dt.day\n",
    "combined_df['month'] = dt.month\n",
    "combined_df['year'] = dt.year\n",
    "combined_df['hour'] = dt.hour\n",
    "combined_df['dayofweek'] = dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating new columns transforming bike ridership to log\n",
    "for column in ['casual', 'registered', 'count']:\n",
    "    combined_df['%s_log' % column] = np.log(combined_df[column] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# mark peak hours\n",
    "#combined_df['peak'] = combined_df[['hour', 'workingday']].apply(lambda x: (0, 1)[(x['workingday'] == 1 and  ( x['hour'] == 8 or 17 <= x['hour'] <= 18 or 12 <= x['hour'] <= 12)) or (x['workingday'] == 0 and  10 <= x['hour'] <= 19)], axis = 1)\n",
    "\n",
    "# mark peak hours\n",
    "# sat/sun - 10am to 7pm\n",
    "# mon-fri - 6am to 10am | 4pm to 7pm\n",
    "combined_df['peak'] = 0\n",
    "combined_df['peak'][(\n",
    "        ( (combined_df['workingday'] == 0 ) & ( (combined_df['hour'] >= 10) & (combined_df['hour'] <= 19) ) ) |\n",
    "        ( \n",
    "            (combined_df['workingday'] == 1 ) & \n",
    "            ( \n",
    "               ( (combined_df['hour'] >= 6) & (combined_df['hour'] <= 10) ) | \n",
    "               ( (combined_df['hour'] >= 16) & (combined_df['hour'] <= 19) )\n",
    "            )\n",
    "        )\n",
    "    )] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defined perfect weather and humid weather variables\n",
    "combined_df['perfect'] = combined_df[['temp', 'windspeed']].apply(lambda x: (0, 1)[x['temp'] > 27 and x['windspeed'] < 30], axis = 1)\n",
    "combined_df['dayhumid'] = combined_df[['humidity', 'workingday']].apply(lambda x: (0, 1)[x['workingday'] == 1 and x['humidity'] >= 60], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_data</th>\n",
       "      <th>atemp</th>\n",
       "      <th>casual</th>\n",
       "      <th>count</th>\n",
       "      <th>datetime</th>\n",
       "      <th>holiday</th>\n",
       "      <th>humidity</th>\n",
       "      <th>registered</th>\n",
       "      <th>season</th>\n",
       "      <th>temp</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>casual_log</th>\n",
       "      <th>registered_log</th>\n",
       "      <th>count_log</th>\n",
       "      <th>peak</th>\n",
       "      <th>perfect</th>\n",
       "      <th>dayhumid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-01 00:00:00</th>\n",
       "      <td>train</td>\n",
       "      <td>14.395</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 01:00:00</th>\n",
       "      <td>train</td>\n",
       "      <td>13.635</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 02:00:00</th>\n",
       "      <td>train</td>\n",
       "      <td>13.635</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>3.332205</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 03:00:00</th>\n",
       "      <td>train</td>\n",
       "      <td>14.395</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 04:00:00</th>\n",
       "      <td>train</td>\n",
       "      <td>14.395</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     _data   atemp  casual  count             datetime  \\\n",
       "2011-01-01 00:00:00  train  14.395       3     16  2011-01-01 00:00:00   \n",
       "2011-01-01 01:00:00  train  13.635       8     40  2011-01-01 01:00:00   \n",
       "2011-01-01 02:00:00  train  13.635       5     32  2011-01-01 02:00:00   \n",
       "2011-01-01 03:00:00  train  14.395       3     13  2011-01-01 03:00:00   \n",
       "2011-01-01 04:00:00  train  14.395       0      1  2011-01-01 04:00:00   \n",
       "\n",
       "                     holiday  humidity  registered  season  temp    ...     \\\n",
       "2011-01-01 00:00:00        0        81          13       1  9.84    ...      \n",
       "2011-01-01 01:00:00        0        80          32       1  9.02    ...      \n",
       "2011-01-01 02:00:00        0        80          27       1  9.02    ...      \n",
       "2011-01-01 03:00:00        0        75          10       1  9.84    ...      \n",
       "2011-01-01 04:00:00        0        75           1       1  9.84    ...      \n",
       "\n",
       "                     month  year  hour dayofweek  casual_log  registered_log  \\\n",
       "2011-01-01 00:00:00      1  2011     0         5    1.386294        2.639057   \n",
       "2011-01-01 01:00:00      1  2011     1         5    2.197225        3.496508   \n",
       "2011-01-01 02:00:00      1  2011     2         5    1.791759        3.332205   \n",
       "2011-01-01 03:00:00      1  2011     3         5    1.386294        2.397895   \n",
       "2011-01-01 04:00:00      1  2011     4         5    0.000000        0.693147   \n",
       "\n",
       "                     count_log  peak  perfect  dayhumid  \n",
       "2011-01-01 00:00:00   2.833213     0        0         0  \n",
       "2011-01-01 01:00:00   3.713572     0        0         0  \n",
       "2011-01-01 02:00:00   3.496508     0        0         0  \n",
       "2011-01-01 03:00:00   2.639057     0        0         0  \n",
       "2011-01-01 04:00:00   0.693147     0        0         0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chain_rows(df):\n",
    "    chain = []\n",
    "    for x in df:\n",
    "        for y in x:\n",
    "            chain.append(y)\n",
    "\n",
    "    return np.array(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RMSLE score function for testing\n",
    "def RMSLE_score(Y_pred, Y_act ):\n",
    "    diff = (np.log(Y_pred + 1) - np.log(Y_act + 1))\n",
    "    mean_error = np.square(diff).mean()\n",
    "    return np.sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get training data\n",
    "def get_train_data():\n",
    "    train_data = combined_df[combined_df['_data'] == 'train'].copy()\n",
    "    return train_data\n",
    "\n",
    "# get test data\n",
    "def get_test_data():\n",
    "    test_data = combined_df[combined_df['_data'] == 'test'].copy()\n",
    "    return test_data\n",
    "\n",
    "# split train and test data\n",
    "def split_train_test(df, cutoff_day = 15):\n",
    "    train_data = df[df['day'] <= cutoff_day]\n",
    "    test_data = df[df['day'] > cutoff_day]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# prepare data for training the model\n",
    "def prepare_data(df, features):\n",
    "    X = df[features].as_matrix()\n",
    "    Y_reg = df['registered_log'].as_matrix()\n",
    "    Y_cas = df['casual_log'].as_matrix()\n",
    "\n",
    "    return X, Y_reg, Y_cas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_kaggle_submission(predictions, file_name):\n",
    "    print \"-\" * 80\n",
    "\n",
    "    # check shape of the test and prediction sets\n",
    "    print \"Generating file for Kaggle Submission File: %s\" % (file_name)\n",
    "    print \"Shape of Kaggle Test Data: \", FINAL_TEST_DF.shape\n",
    "    print \"Shape of Kaggle Test Predictions: \", predictions.shape  \n",
    "\n",
    "    # formatting predictions to integers and removing negative values\n",
    "    predictions = np.rint(predictions)\n",
    "    predictions[ predictions < 0] = 0\n",
    "    print predictions\n",
    "    \n",
    "    print \"Shape of Final Predictions: \", predictions.shape\n",
    "\n",
    "    # create submission file\n",
    "    #sbmt_file_name = [os.getcwd(),'../submissions/',file_name]\n",
    "    sbmt_file_name = file_name\n",
    "    np.savetxt(sbmt_file_name, zip(FINAL_TEST_DF['datetime'], predictions), delimiter=',', fmt=\"%s\", header=','.join(['datetime','count']), comments='')\n",
    "    print \"kaggle submission file generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FINAL_TEST_DF = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    'weather', 'temp', 'atemp', 'windspeed','season',\n",
    "    'workingday',  'holiday', 'humidity', \n",
    "    'hour', 'dayofweek', 'year', \n",
    "    'peak', 'perfect', 'dayhumid'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prediction on validation data\n",
    "def predict_validation_data(df, model, features):\n",
    "    train, test = split_train_test(df)\n",
    "\n",
    "    X_train, Y_train_reg, Y_train_cas = prepare_data(train, features)\n",
    "    X_test, Y_test_reg, Y_test_cas = prepare_data(test, features)\n",
    "\n",
    "    # predict registered users count\n",
    "    model_reg = model.fit(X_train, Y_train_reg)\n",
    "    Y_prd_reg = np.exp(model_reg.predict(X_test)) - 1\n",
    "\n",
    "    # predict casual users count\n",
    "    model_cas = model.fit(X_train, Y_train_cas)\n",
    "    Y_prd_cas = np.exp(model_cas.predict(X_test)) - 1\n",
    "\n",
    "    # combine registered and casual user predictions\n",
    "    Y_prd = np.round(Y_prd_reg + Y_prd_cas)\n",
    "    Y_prd[Y_prd < 0] = 0\n",
    "\n",
    "    # transform predictions back from log\n",
    "    Y_test = np.exp(Y_test_reg) + np.exp(Y_test_cas) - 2\n",
    "\n",
    "    score = RMSLE_score(Y_prd, Y_test)\n",
    "    return (Y_prd, Y_prd_reg, Y_prd_cas, Y_test, score)\n",
    "\n",
    "# predict Kaggle test data & transform output\n",
    "def predict_kaggle_data(train_df, test_df, model, features):\n",
    "    # prepare training data\n",
    "    X_train, Y_train_reg, Y_train_cas = prepare_data(train_df, features)\n",
    "\n",
    "    # prepare test data\n",
    "    X_test = test_df[features].as_matrix()\n",
    "\n",
    "    # predict casual users count\n",
    "    model_cas = model.fit(X_train, Y_train_cas)\n",
    "    Y_prd_cas = np.exp(model_cas.predict(X_test)) - 1\n",
    "    \n",
    "    # predict registered users count\n",
    "    model_reg = model.fit(X_train, Y_train_reg)\n",
    "    Y_prd_reg = np.exp(model_reg.predict(X_test)) - 1\n",
    "\n",
    "    # combine casual & registered predictions together\n",
    "    Y_prd = np.round(Y_prd_reg + Y_prd_cas)\n",
    "    Y_prd[Y_prd < 0] = 0\n",
    "    \n",
    "    return (Y_prd, Y_prd_reg, Y_prd_cas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 1000, \n",
    "    'max_depth': 15, \n",
    "    'random_state': 0, \n",
    "    'min_samples_split' : 5, \n",
    "    'n_jobs': -1}\n",
    "\n",
    "rf_model = RandomForestRegressor(**params)\n",
    "rf_features = [\n",
    "    'weather', 'temp', 'atemp', 'windspeed',\n",
    "    'workingday', 'season', 'holiday', 'dayhumid',\n",
    "    'hour', 'dayofweek', 'peak'\n",
    "]\n",
    "\n",
    "(rf_prd, rf_prd_reg, rf_prd_cas, rf_test, rf_score) = predict_validation_data(get_train_data(), rf_model, rf_features)\n",
    "print rf_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 150, \n",
    "    'max_depth': 5, \n",
    "    'random_state': 0, \n",
    "    'min_samples_leaf' : 10, \n",
    "    'learning_rate': 0.1, \n",
    "    'subsample': 0.7, \n",
    "    'loss': 'ls'}\n",
    "\n",
    "gbm_model = GradientBoostingRegressor(**params)\n",
    "gbm_features = [\n",
    "    'weather', 'temp', 'atemp', 'windspeed',\n",
    "    'workingday', 'season', 'holiday', 'humidity',\n",
    "    'hour', 'dayofweek', 'year', 'perfect'\n",
    "]\n",
    "\n",
    "(gbm_prd, gbm_prd_reg, gbm_prd_cas, gbm_test, gbm_score) = predict_validation_data(get_train_data(), gbm_model, gbm_features)\n",
    "print gbm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine predictions from both the models\n",
    "# random forest and gradient boost\n",
    "y_prd = np.round(.2 * rf_prd + .8 * gbm_prd)\n",
    "print RMSLE_score(y_prd, rf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on Kaggle Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get train and test data\n",
    "train_df = get_train_data()\n",
    "\n",
    "# predict on Kaggle data using random forest\n",
    "(rf_prd, rf_prd_reg, rf_prd_cas) = predict_kaggle_data(train_df, FINAL_TEST_DF, rf_model, rf_features)\n",
    "\n",
    "# predict on Kaggle data using gradient boost\n",
    "(gbm_prd, gbm_prd_reg, gbm_prd_cas) = predict_kaggle_data(train_df, FINAL_TEST_DF, gbm_model, gbm_features)\n",
    "\n",
    "# combine predictions from both the models\n",
    "# random forest and gradient boost\n",
    "output = np.round(.2 * rf_prd + .8 * gbm_prd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_kaggle_submission(output, 'combine_random_forest_grad_boost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model on Rolling Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_rolling_data(df, data_set = \"train\"):\n",
    "    X = []\n",
    "    df.datetime = pd.to_datetime(df.datetime)\n",
    "    \n",
    "    if ( data_set == \"train\"):\n",
    "        global train_df_rolling\n",
    "\n",
    "        for mon in np.arange(24):\n",
    "            if mon < 12:\n",
    "                date = \"20/{:d}/11\".format(mon+1) \n",
    "            else:\n",
    "                date = \"20/{:d}/12\".format(mon+1-12)\n",
    "\n",
    "            X.append(df.loc[(df['datetime'] < datetime.strptime(date, \"%d/%m/%y\"))])\n",
    "        \n",
    "        train_df_rolling = X\n",
    "    else:\n",
    "        global test_df_rolling\n",
    "\n",
    "        for mon in np.arange(24):\n",
    "            if mon < 12:\n",
    "                year = 2011\n",
    "                month = mon+1\n",
    "            else:\n",
    "                year = 2012\n",
    "                month = mon-11\n",
    "\n",
    "            X.append(df.loc[(df['year'] == year) & (df['month'] == month )])\n",
    "        \n",
    "        test_df_rolling = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare rolling training data set\n",
    "prepare_rolling_data(get_train_data(), \"train\")\n",
    "prepare_rolling_data(get_test_data(), \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['_data', 'atemp', 'casual', 'count', 'datetime', 'holiday',\n",
       "       'humidity', 'registered', 'season', 'temp', 'weather', 'windspeed',\n",
       "       'workingday', 'date', 'day', 'month', 'year', 'hour', 'dayofweek',\n",
       "       'casual_log', 'registered_log', 'count_log', 'peak', 'perfect',\n",
       "       'dayhumid'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_rolling[0].columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['_data', 'atemp', 'casual', 'count', 'datetime', 'holiday',\n",
       "       'humidity', 'registered', 'season', 'temp', 'weather', 'windspeed',\n",
       "       'workingday', 'date', 'day', 'month', 'year', 'hour', 'dayofweek',\n",
       "       'casual_log', 'registered_log', 'count_log', 'peak', 'perfect',\n",
       "       'dayhumid'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_rolling[0].columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest on Rolling Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdl_rf_cas = []\n",
    "mdl_rf_reg = []\n",
    "prd_rf_cas = []\n",
    "prd_rf_reg = []\n",
    "prd_rf_cnt = []\n",
    "prd_rf_val_cnt = []\n",
    "\n",
    "MAX_MONTHS = 2\n",
    "\n",
    "params = {\n",
    "    'n_estimators': 1000, \n",
    "    'max_depth': 15, \n",
    "    'random_state': 0, \n",
    "    'min_samples_split' : 5, \n",
    "    'n_jobs': -1}\n",
    "\n",
    "rf_model = RandomForestRegressor(**params)\n",
    "rf_features = [\n",
    "    'weather', 'temp', 'atemp', 'windspeed',\n",
    "    'workingday', 'season', 'holiday', 'dayhumid',\n",
    "    'hour', 'dayofweek', 'peak'\n",
    "]\n",
    "\n",
    "for m in np.arange(MAX_MONTHS):\n",
    "    (rf_prd, rf_prd_reg, rf_prd_cas, rf_test, rf_score) = predict_validation_data(train_df_rolling[m], rf_model, rf_features)\n",
    "    prd_rf_val_cnt.append(rf_prd)\n",
    "    (rf_prd, rf_prd_reg, rf_prd_cas) = predict_kaggle_data(train_df_rolling[m], test_df_rolling[m], rf_model, rf_features)\n",
    "    prd_rf_cnt.append(rf_prd)\n",
    "    prd_rf_reg.append(rf_prd_reg)\n",
    "    prd_rf_cas.append(rf_prd_cas)\n",
    "    print m, rf_score, prd_rf_cnt[m].shape\n",
    "    \n",
    "    '''\n",
    "    for n in np.arange(m+1, MAX_MONTHS):\n",
    "        #print \"BEFORE = m+1:{} | train_X:{} | train_Y_cas:{} | train_Y_reg:{}\".format(m+1, len(train_X[m+1]), len(train_Y_cas[m+1]), len(train_Y_reg[m+1]))\n",
    "        train_df_rolling[n] = train_df_rolling[n].append(test_df_rolling[m])\n",
    "        #print train_df_rolling[n][(train_df_rolling[n]['_data'] == 'test')]\n",
    "        train_df_rolling[n].loc[(train_df_rolling[n]['_data'] == 'test'), 'casual'].map(lambda x <= pd.Series(prd_rf_cas[m])\n",
    "        train_df_rolling[n].loc[(train_df_rolling[n]['_data'] == 'test'), 'registered'] = pd.Series(prd_rf_reg[m])\n",
    "        train_df_rolling[n].loc[(train_df_rolling[n]['_data'] == 'test'), 'casual_log'] = pd.Series(np.log(prd_rf_cas[m] + 1))\n",
    "        train_df_rolling[n].loc[(train_df_rolling[n]['_data'] == 'test'), 'registered_log'] = pd.Series(np.log(prd_rf_reg[m] + 1))\n",
    "\n",
    "        #print \"AFTER  = m+1:{} | train_X:{} | train_Y_cas:{} | train_Y_reg:{}\".format(m+1, len(train_X[m+1]), len(train_Y_cas[m+1]), len(train_Y_reg[m+1]))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_val_predictions = chain_rows(prd_rf_val_cnt)\n",
    "rf_predictions = chain_rows(prd_rf_cnt)\n",
    "len(rf_predictions), sum(rf_predictions<0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost on Rolling Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdl_gbm_cas = []\n",
    "mdl_gbm_reg = []\n",
    "prd_gbm_cas = []\n",
    "prd_gbm_reg = []\n",
    "prd_gbm_cnt = []\n",
    "prd_gbm_val_cnt = []\n",
    "MAX_MONTHS = 24\n",
    "\n",
    "params = {\n",
    "    'n_estimators': 150, \n",
    "    'max_depth': 5, \n",
    "    'random_state': 0, \n",
    "    'min_samples_leaf' : 10, \n",
    "    'learning_rate': 0.1, \n",
    "    'subsample': 0.7, \n",
    "    'loss': 'ls'}\n",
    "\n",
    "gbm_model = GradientBoostingRegressor(**params)\n",
    "\n",
    "gbm_features = [\n",
    "    'weather', 'temp', 'atemp', 'windspeed',\n",
    "    'workingday', 'season', 'holiday', 'humidity',\n",
    "    'hour', 'dayofweek', 'year', 'perfect'\n",
    "]\n",
    "\n",
    "(gbm_prd, gbm_prd_reg, gbm_prd_cas, gbm_test, gbm_score) = predict_validation_data(get_train_data(), gbm_model, gbm_features)\n",
    "\n",
    "for m in np.arange(MAX_MONTHS):\n",
    "    (gbm_prd, gbm_prd_reg, gbm_prd_cas, gbm_test, gbm_score) = predict_validation_data(train_df_rolling[m], gbm_model, gbm_features)\n",
    "    prd_gbm_val_cnt.append(rf_prd)\n",
    "    (gbm_prd, gbm_prd_reg, gbm_prd_cas) = predict_kaggle_data(train_df_rolling[m], test_df_rolling[m], gbm_model, gbm_features)\n",
    "    prd_gbm_cnt.append(gbm_prd)\n",
    "    prd_gbm_reg.append(gbm_prd_reg)\n",
    "    prd_gbm_cas.append(gbm_prd_cas)\n",
    "    print m, gbm_score, prd_gbm_cnt[m].shape\n",
    "        \n",
    "    '''\n",
    "    for n in np.arange(m+1, MAX_MONTHS):\n",
    "        #print \"BEFORE = m+1:{} | train_X:{} | train_Y_cas:{} | train_Y_reg:{}\".format(m+1, len(train_X[m+1]), len(train_Y_cas[m+1]), len(train_Y_reg[m+1]))\n",
    "        train_df_rolling[1][(train_df_rolling[1]['_data'] == 'test')]['casual'] = pd.Series(prd_gbm_cas[m])\n",
    "        train_df_rolling[1][(train_df_rolling[1]['_data'] == 'test')]['registered'] = pd.Series(prd_gbm_reg[m])\n",
    "        train_df_rolling[1][(train_df_rolling[1]['_data'] == 'test')]['casual_log'] = pd.Series(np.log(prd_gbm_cas[m] + 1))\n",
    "        train_df_rolling[1][(train_df_rolling[1]['_data'] == 'test')]['registered_log'] = pd.Series(np.log(prd_gbm_reg[m] + 1))\n",
    "        #print \"AFTER  = m+1:{} | train_X:{} | train_Y_cas:{} | train_Y_reg:{}\".format(m+1, len(train_X[m+1]), len(train_Y_cas[m+1]), len(train_Y_reg[m+1]))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbm_val_predictions = chain_rows(prd_gbm_val_cnt)\n",
    "gbm_predictions = chain_rows(prd_gbm_cnt)\n",
    "len(gbm_predictions), sum(gbm_predictions<0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine predictions from both the models\n",
    "# random forest and gradient boost\n",
    "y_val_prd = np.round(.2 * rf_val_predictions + .8 * gbm_val_predictions)\n",
    "print RMSLE_score(y_prd, rf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Rolling Predictions with Combined Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine predictions from both the models\n",
    "# random forest and gradient boost\n",
    "output = np.round(.2 * rf_predictions + .8 * gbm_predictions)\n",
    "\n",
    "print output\n",
    "print output.shape, sum(output<0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Submission with Rolling Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_kaggle_submission(output, 'combine_rolling_random_forest_grad_boost.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
